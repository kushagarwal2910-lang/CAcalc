{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfIA51M3BSsQRYqCQmZnWW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kushagarwal2910-lang/CAcalc/blob/main/pytorch%2B%2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch #gives the core functionalities of pytorch\n",
        "import torch.nn as nn #components for building neural networks\n",
        "import torch.optim as optim #tools for training those networks"
      ],
      "metadata": {
        "id": "AP4NcUghJcqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tensors"
      ],
      "metadata": {
        "id": "7BPuQgqY8hpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distances = torch.tensor([[1.0],[2.0],[3.0],[4.0]], dtype=torch.float32)\n",
        "times = torch.tensor([[6.96],[12.11],[16.77],[22.21]], dtype=torch.float32)\n",
        "#here in tensor we created a nested square bracket\n",
        "#outer square bracket called batch\n",
        "#inner square brackets called sample"
      ],
      "metadata": {
        "id": "Ovkndjia8ERo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "Use float32 when:\n",
        "\n",
        "  .)Debugging\n",
        "\n",
        "  .)Small models\n",
        "\n",
        "  .)Scientific precision matters\n",
        "\n",
        "  .)Loss suddenly becomes NaN\n",
        "\n",
        "Use bfloat16 when:\n",
        "\n",
        "  .)Training large models\n",
        "\n",
        "  .)Memory is tight\n",
        "\n",
        "  .)Speed matters\n",
        "\n",
        "  .)Using TPUs or modern GPUs\n"
      ],
      "metadata": {
        "id": "O9EUTq8c_1yI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(6).reshape(2,3)\n",
        "#creating its transpose\n",
        "y = x.t()"
      ],
      "metadata": {
        "id": "9zMdFUdt9g5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Contiguous = stored continuously in memory in the expected order.\n",
        "e = y.is_contiguous()\n",
        "e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMrOwOw5A0RV",
        "outputId": "7ba77170-bd19-49c6-ecc5-22327ebb496f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to make the matrice contiguous\n",
        "y = y.contiguous()\n",
        "y.is_contiguous()\n",
        "#always check .is_contiguous() before low level memeory operation like view()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSnpSom2BaPL",
        "outputId": "6c5b75ec-0880-4ac5-e6bb-dd96957e9494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generating a random tensor\n",
        "torch.randn(2,12)#generates a 2d tensor(matrice) of size 2x12 with filling random elements\n",
        "#if we wnt we can write torch.randn(x,y,z,w) then it gives a 4d vector"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCyjR7k2esHq",
        "outputId": "849e1eb5-06d8-4b4d-dd9d-f677c9d47923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0180,  0.5418, -0.3213,  0.0197,  0.4218,  1.5255,  1.5247,  0.5550,\n",
              "          0.4713,  0.1555,  1.1078,  0.9898],\n",
              "        [ 0.8861, -0.5836, -0.9392,  1.6625, -0.0998,  0.7089, -0.1322,  0.6406,\n",
              "          0.4187, -0.6999, -1.0402, -0.2785]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#broadcasting-it allows you to perform math b/w tensors of difft shapes without copying data\n",
        "#broadcasting virtually expands smaller tensor\n",
        "\n",
        "#batch of 32 images, 3 rgb channels, 64x64 pixels\n",
        "images = torch.randn(32,3,64,64)\n",
        "\n",
        "#if we want to normalize each channel (r,g,b) with a specific mean\n",
        "mean = torch.tensor([0.5,0.4, 0.3]) #shape: 3\n",
        "\n",
        "#View mean as (1,3,1,1) so it broadcasts over batch, H and W\n",
        "mean = mean.view(1,3,1,1)\n",
        "#this changes the dimension of mean tensor from 1d to 4d as of images named tensor\n",
        "#dimensionized so that it can perform operations with images named tensor\n",
        "\n",
        "#pytorch virtually expands 'mean' to (32,3,64,64) instantly\n",
        "normalized = images-mean"
      ],
      "metadata": {
        "id": "R3TWp4ftBylR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#engiene matrix operation\n",
        "# matmul is the standard dot product\n",
        "# torch.bmm is batch matrix multiply, it ignores broadcasting rules for safety\n",
        "\n",
        "# Batch of 10 users, 5 items per user, embedding size 64\n",
        "User_Embeddings = torch.randn(10, 5, 64)\n",
        "\n",
        "# Transposed Item Embeddings: Batch 10, 64 embedding size, 5 items\n",
        "Item_Embeddings_T = torch.randn(10, 64, 5)\n",
        "\n",
        "# Calculate scores for every user-item pair in the batch\n",
        "# (10, 5, 64) @ (10, 64, 5) -> (10, 5, 5)\n",
        "scores = torch.bmm(User_Embeddings, Item_Embeddings_T)"
      ],
      "metadata": {
        "id": "oepTf90ReIcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#squeeze and unsqueeze\n",
        "x = torch.tensor([2,0,3])\n",
        "x.shape\n",
        "\n",
        "#unsqeze adds a axis with element 1 into the place where the dimension is assigned in brackets\n",
        "#x.unsqueeze(dimension where the axis with element 1 to be added in x)\n",
        "x = x.unsqueeze(0)#becomes row vector(1,3) this one in (1,3) is the axis added by unsqueeze\n",
        "x = x.unsqueeze(1)#becomes column vector(3,1) this one in (3,1) is the axis added by unsqueeze\n",
        "x = x.unsqueeze(2)#becomes 3d (3,1,1) this one in (3,1,1) is the axis added by unsqueeze\n",
        "#note you cant directly jump from initial x to the 3d tensor of x\n",
        "#you need to go through 1d then 2d then come to 3d\n",
        "\n",
        "y = torch.tensor([2,3,4,1,3])\n",
        "\n",
        "#squeeze removes the axis with dimension 1 if size > 1 then cant remove\n",
        "y = y.squeeze(0) #dosent work as the dimension 0 have 3 as size\n",
        "\n",
        "z = torch.tensor([[1],[2]])\n",
        "\n",
        "z = z.squeeze(1)#as now the sizeof z is (2,1)\n",
        "z"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D5G-8IPhQAT",
        "outputId": "e016cca2-b8c7-4515-ddab-a3712868df35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Advanced Indexing\n",
        "\n",
        "#gather\n",
        "'''\n",
        "torch.gather(input, dim, index, *, sparse_grad=False, out=None)\n",
        "\n",
        ".)input: The tensor from which you'll be extracting elements. Think of this as your source of data.\n",
        "\n",
        ".)dim: The dimension along which you'll be gathering elements. If your tensor is a matrix (2D), this could be either the rows\n",
        "  (dimension 0) or the columns (dimension 1). For higher-dimensional tensors, you have more choices.\n",
        "\n",
        ".)index: This tensor holds the indices of the elements you want. Its shape should be compatible with the input tensor and the\n",
        " chosen dimension.\n",
        "\n",
        ".)sparse_grad: (Optional) If True, the gradient with respect to the input will be a sparse tensor.\n",
        "\n",
        ".)out: (Optional) The destination tensor.\n",
        "'''\n",
        "probs = torch.tensor([[0.1, 0.2, 0.7, 0.0],  # Model guessed class 2\n",
        "                      [0.8, 0.1, 0.1, 0.0],  # Model guessed class 0\n",
        "                      [0.0, 0.0, 0.1, 0.9]]) # Model guessed class 3\n",
        "\n",
        "targets = torch.tensor([2, 0, 3]).unsqueeze(1) # Shape (3, 1)\n",
        "\n",
        "selected_probs = torch.gather(input=probs, dim=1, index=targets)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#scatter\n",
        "\n",
        "'''\n",
        "torch.scatter writes values into a tensor at specific coordinates\n",
        "defined by an index tensor (commonly used to create One-Hot vectors).\n",
        "\n",
        "torch.scatter(input, dim, index, src, *, reduce='add', sparse_grad=False)\n",
        "\n",
        ".)src: This is the source tensor containing the values that you want to scatter into the input\n",
        " tensor. Its shape must be compatible with the index tensor and the input tensor's dimensions.\n",
        "'''\n",
        "input_tensor = torch.zeros(3, 4)\n",
        "index_tensor = torch.tensor([[0, 2],    # In row 0, scatter to column 0 and 2\n",
        "                             [1, 3],    # In row 1, scatter to column 1 and 3\n",
        "                             [0, 1]])   # In row 2, scatter to column 0 and 1\n",
        "src_tensor = torch.tensor([[10., 20.],  # Values to scatter for row 0\n",
        "                           [30., 40.],  # Values to scatter for row 1\n",
        "                           [50., 60.]]) # Values to scatter for row 2\n",
        "\n",
        "output_tensor = torch.scatter(input=input_tensor, dim=1, index=index_tensor, src=src_tensor)\n",
        "\n"
      ],
      "metadata": {
        "id": "B6Bx-3QvjBqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#einstein summation\n",
        "'''\n",
        "logic of syntax:#\n",
        "1.)Indices (i, j, k, ...): Each letter represents a dimension (axis) of your tensor.\n",
        "2.)Comma (, ): Separates different input tensors.\n",
        "3.)Arrow (->): Separates the input structure from the desired output structure.\n",
        "4.)Summation Rule: Any index found on the left side (input) but missing on the right side (output) is summed over (reduced).\n",
        "'''\n",
        "\n",
        "#element wise operation\n",
        "a = torch.tensor([1., 2., 3.])\n",
        "b = torch.tensor([4., 5., 6.])\n",
        "\n",
        "out = torch.einsum(\"i,i->i\", a, b)#element wise multiplication happens output is [4,10,18]\n",
        "\n",
        "#dot product(vetor->scalar)\n",
        "out = torch.einsum(\"i,i->\", a, b) #value wil be (1.4 + 2.5 + 3.6 = 32) therefore the scalar result after dot product is 32\n",
        "\n",
        "#outer product(vector -> matrix)\n",
        "a = torch.tensor([1., 2., 3.])\n",
        "b = torch.tensor([4., 5.])\n",
        "\n",
        "out = torch.einsum(\"i,j->ij\", a, b)#output is of size (3,2) i.e [[4,5],[8,10],[12,15]]\n",
        "\n",
        "#vector matrix multiplication\n",
        "A = torch.randn(3,4) #matrice\n",
        "x = torch.randn(4)#treated as column matrix i.e (4,1) its called as a vector\n",
        "\n",
        "out = torch.einsum(\"ij,j->i\", A, x)#now (3,4).(4,1)-> 3 or (3,1) dimension\n",
        "\n",
        "#matrix vector multiplication\n",
        "A = torch.randn(4,3) #matrice\n",
        "x = torch.randn(4)#treated as row matrix i.e (1,4) its called as a vector\n",
        "\n",
        "out = torch.einsum(\"i,ij->j\", x,A)#now (1,4).(4,3)-> 3 or (1,3) dimension\n",
        "\n",
        "#transpose of matrix\n",
        "A = torch.randn(2, 3)\n",
        "\n",
        "out = torch.einsum(\"ij->ji\", A)\n",
        "\n",
        "#matrix multiply\n",
        "A = torch.randn(2, 3)\n",
        "B = torch.randn(3, 4)\n",
        "\n",
        "out = torch.einsum(\"ik,kj->ij\", A, B)\n",
        "\n",
        "#batch matrix multiply\n",
        "A = torch.randn(10, 2, 3)\n",
        "B = torch.randn(10, 3, 4)\n",
        "\n",
        "out = torch.einsum(\"bik,bkj->bij\", A, B)#equivalent to torch.bmm(A, B)\n",
        "\n",
        "#tensor contraction\n",
        "A = torch.randn(3, 4, 5)\n",
        "B = torch.randn(5, 4, 6)\n",
        "\n",
        "out = torch.einsum(\"ijk,kjl->il\", A, B) #dimension become (3,6)\n",
        "\n",
        "#broadcasting multiplication\n",
        "X = torch.randn(5, 3)\n",
        "w = torch.randn(3)\n",
        "\n",
        "out = torch.einsum(\"ij,j->ij\", X, w)#dimension is (5,3)\n",
        "\n",
        "#weighted sum over dimension\n",
        "out = torch.einsum(\"ij,j->i\", X, w)\n",
        "\n",
        "#summation over one axis\n",
        "X = torch.arange(4).resize(1,4)\n",
        "out = torch.einsum(\"ij->j\", X)#equivalent to X.sum(dim=0), output dimension matrice is of 4\n",
        "#output is tensor([0, 1, 2, 3])\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "out = torch.einsum(\"ij->i\", X)#equivalent to X.sum(dim=1), output dimension matrice is of 1\n",
        "#output is tensor([6])\n",
        "\n",
        "#permuting high dimensional tensors\n",
        "X = torch.randn(2, 3, 4)\n",
        "out = torch.einsum(\"abc->bca\", X)#out dimension is (3,4,2)"
      ],
      "metadata": {
        "id": "RvDphFkOHLuM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d4ded3d-2a2e-4c11-d7fa-7047e5876207"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py:1002: UserWarning: non-inplace resize is deprecated\n",
            "  warnings.warn(\"non-inplace resize is deprecated\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stride\n",
        "'''\n",
        "Stride tells PyTorch how to read data from memory without copying it.\n",
        "\n",
        "Why copying is a BIG problem in LLMs:\n",
        "  Imagine training an LLM:\n",
        "  Billions of parameters\n",
        "  Huge tensors: (batch, sequence, heads, dim)\n",
        "  Every layer does:\n",
        "  reshape\n",
        "  transpose\n",
        "  split into heads\n",
        "  merge heads back\n",
        "If every reshape copied data:\n",
        "  GPU memory would explode\n",
        "  Training would be 10–50× slower\n",
        "  LLM training would be impossible\n",
        "Stride exists to avoid copying.\n",
        "\n",
        "torch.stride() is never directly used in deeplearning but we use it's outcoe cases like\n",
        "  view()\n",
        "  reshape()\n",
        "  permute()\n",
        "  transpose()\n",
        "  einsum()\n",
        "  All of these depend on stride internally.\n",
        "'''\n",
        "\n",
        "#view- used to resize a tensor or flatten a tensor most commonly\n",
        "# Example: Output from a Convolution layer\n",
        "# Shape: (Batch_Size=64, Channels=32, Height=10, Width=10)\n",
        "conv_output = torch.rand(64, 32, 10, 10)\n",
        "\n",
        "# We want to flatten this to (Batch_Size, Everything_Else)\n",
        "# -1 tells PyTorch to calculate that dimension automatically\n",
        "flattened = conv_output.view(64, -1)\n",
        "\n",
        "print(flattened.shape)\n",
        "# Output: torch.Size([64, 3200]) -> Ready for Linear Layer\n",
        "\n",
        "\n",
        "\n",
        "#reshape()\n",
        "'''does almost the same thing as view(), but it is robust. If the data is contiguous, it returns a view (fast).\n",
        "If the data is not contiguous, it automatically copies the data into a new chunk of memory (safe but slightly slower).\n",
        "\n",
        "Use reshape() when you just want the code to work and don't want to worry about memory contiguity errors.\n",
        "'''\n",
        "# Create a tensor and transpose it (breaks memory contiguity)\n",
        "x = torch.rand(2, 3).transpose(0, 1)\n",
        "# x.view(6) -> This would RAISE AN ERROR because x is not contiguous\n",
        "y = x.reshape(6) # This WORKS (it copies data if needed)\n",
        "print(y)\n",
        "\n",
        "#permute\n",
        "# Image loaded from disk: (Batch=10, Height=256, Width=256, RGB_Channels=3)\n",
        "raw_image_batch = torch.rand(10, 256, 256, 3)\n",
        "# We need to move the Channels (index 3) to index 1.\n",
        "# New order indices: 0 (Batch), 3 (Channels), 1 (Height), 2 (Width)\n",
        "pytorch_ready = raw_image_batch.permute(0, 3, 1, 2)\n",
        "print(pytorch_ready.shape)\n",
        "# Output: torch.Size([10, 3, 256, 256]) -> Ready for Conv2d\n",
        "\n",
        "#transpose\n",
        "#Syntax: tensor.transpose(dim0, dim1)\n",
        "# Data format: (Batch_Size=32, Sequence_Length=100, Embedding_Dim=512)\n",
        "nlp_data = torch.rand(32, 100, 512)\n",
        "# Swap Batch (0) and Sequence (1) for a specific LSTM implementation\n",
        "lstm_input = nlp_data.transpose(0, 1)\n",
        "print(lstm_input.shape)\n",
        "# Output: torch.Size([100, 32, 512])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OoIeZSzMBwz",
        "outputId": "a444c126-4d93-4f02-ad6d-d33a504c28d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3200])\n",
            "tensor([0.8147, 0.3270, 0.1894, 0.2650, 0.3533, 0.3404])\n",
            "torch.Size([10, 3, 256, 256])\n",
            "torch.Size([100, 32, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autograd and data"
      ],
      "metadata": {
        "id": "HinqGYO-yIEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Ground rule (mental model):\n",
        "  .Every Tensor with requires_grad=True is a node\n",
        "  .Every operation creates an edge\n",
        "  .Backward = graph traversal (reverse topological order)\n",
        "  .Graph is dynamic (rebuilt every forward pass)\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "What is the Computation Graph really?\n",
        "\n",
        "A computation graph is just:\n",
        "A record of “who produced whom” during computation.\n",
        "example:\n",
        "y = x * 2\n",
        "z = y + 3\n",
        "pytorch internally records [x → (multiply) → y → (add) → z]\n",
        "\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "What is a node?\n",
        "\n",
        "A node is:\n",
        "A tensor or an operation result that exists in the graph\n",
        "Examples of nodes: x,y,z\n",
        "Each node:\n",
        "  Knows how it was created\n",
        "  Knows what came before it\n",
        "In PyTorch:\n",
        "  Every tensor created via an operation becomes a node.\n",
        "  Leaf tensors are special.\n",
        "\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "What is an edge?\n",
        "\n",
        "An edge means:\n",
        "“This tensor depends on that tensor”\n",
        "Example:\n",
        "y = x * 2\n",
        "graphically: x ──edge──> y\n",
        "That edge means:\n",
        "  If y changes\n",
        "  x influenced it\n",
        "So when gradients flow:\n",
        "  They travel backward along edges\n",
        "\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "what is DAG?\n",
        "\n",
        "directed(D): have direction\n",
        "acyclic(A): No loops\n",
        "Graph(G)\n",
        "\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "Why do gradients “flow”?\n",
        "\n",
        "Because each operation knows:\n",
        "“If output gradient is g, how does input gradient look?”\n",
        "example: y = x * 2 --------------> ∂y/∂x = 2\n",
        "\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "leaf tenor?\n",
        "\n",
        "it is a tensor created directly by you, not by an operation.\n",
        "example:\n",
        "  x = torch.tensor(3.0, requires_grad=True)\n",
        "  here x is a leaf tensor.\n",
        "\n",
        "  y = x * 2\n",
        "  here y is not a leaf tensor.\n",
        "Why does this matter?\n",
        "  Only leaf tensors:\n",
        "  Store gradients in .grad\n",
        "  Are considered parameters (weights)\n",
        "  hence according to example x.grad exists but y.grad doesnt exist.\n",
        "\n",
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "'''\n",
        "#smallest possible graph\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x * 3\n",
        "y.backward()\n",
        "print(x.grad)  # 3\n",
        "\n",
        "#check graph connection\n",
        "print(y.grad_fn)\n",
        "\n",
        "#linear chain graph\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x * 2\n",
        "z = y + 3\n",
        "loss = z ** 2\n",
        "loss.backward()\n",
        "print(x.grad)\n",
        "'''on doing a loss.backward the gradients are calculated of loss then it flows to the Z then it flows to Y\n",
        "then it flows to X and it is then the gradient is then stored into X.grad and it is stored in such a way as displayed below\n",
        "\n",
        "∂loss/∂z = 14\n",
        "∂z/∂y = 1\n",
        "∂y/∂x = 2\n",
        "\n",
        "x.grad = 14 * 1 * 2 = 28\n",
        "\n",
        "'''\n",
        "\n",
        "#inspect backward graph structure\n",
        "print(y.grad_fn)# it tells you which operation produced the tensor\n",
        "\n",
        "#leaf vs non-leaf\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "y = x * 2\n",
        "print(x.is_leaf)  # True\n",
        "print(y.is_leaf)  # False\n",
        "\n",
        "#intermedite gradients\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "y = x * 2\n",
        "y.retain_grad()#it is like .grad() which was for leaf tensors similarly .retain_grad() is for non-leaf tensors\n",
        "z = y.sum()\n",
        "z.backward()\n",
        "print(y.grad)\n",
        "\n",
        "#explicit\n",
        "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
        "y = x * 2\n",
        "y.backward(torch.tensor([1., 1., 1.]))\n",
        "print(x.grad)\n",
        "\n",
        "#dynamic graph (runtime decision)\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "if x > 0:\n",
        "    y = x * 5\n",
        "else:\n",
        "    y = x / 5\n",
        "y.backward()\n",
        "print(x.grad)\n",
        "\n",
        "#graph destroyed after backward\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x * x\n",
        "y.backward()\n",
        "# y.backward()  # RuntimeError\n",
        "\n",
        "#cut edge\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "y = x * 2\n",
        "z = y.detach() * 3\n",
        "z.requires_grad = True\n",
        "z.backward()\n",
        "print(x.grad)  # None\n",
        "'''\n",
        "x ──X──▶ y    y_detached ──▶ z\n",
        "\n",
        "y.detach() is used when you want to use the value of y but completely stop gradients from flowing back through it.\n",
        "\n",
        "or\n",
        "\n",
        "y.detach() keeps the number the same, but tells PyTorch: “do not learn from this.”\n",
        "'''\n",
        "\n",
        "#no grad\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "with torch.no_grad():\n",
        "    y = x * 4\n",
        "print(y.requires_grad)#it is lik a flag telling autograd telling Do I care about gradients flowing through this tensor?\n",
        "#we can turn it of manually as y.requires_grad_(False)\n",
        "\n",
        "#matrix dag\n",
        "X = torch.randn(4, 3, requires_grad=True)\n",
        "W = torch.randn(2, 3, requires_grad=True)\n",
        "Y = X @ W.T\n",
        "loss = Y.sum()\n",
        "loss.backward()\n",
        "print(X.grad.shape)\n",
        "print(W.grad.shape)\n",
        "\n",
        "#autograd.grad\n",
        "x = torch.tensor(4.0, requires_grad=True)\n",
        "y = x ** 3\n",
        "grad = torch.autograd.grad(y, x)\n",
        "print(grad)\n",
        "\n",
        "#higher order gradient\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "y = x ** 2\n",
        "grad1 = torch.autograd.grad(y, x, create_graph=True)[0]\n",
        "grad2 = torch.autograd.grad(grad1, x)[0]\n",
        "print(grad1)  # first derivative\n",
        "print(grad2)  # second derivative\n",
        "\n",
        "#freeze part of graph\n",
        "W = torch.randn(3, requires_grad=True)\n",
        "b = torch.randn(3)\n",
        "x = torch.randn(3)\n",
        "y = W * x + b\n",
        "loss = y.sum()\n",
        "loss.backward()\n",
        "print(W.grad)\n",
        "print(b.grad)  # None\n"
      ],
      "metadata": {
        "id": "Gc09JRN-X1xy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "096d51b5-a32a-4d32-8eba-9a8b1c280ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.)\n",
            "<MulBackward0 object at 0x7e8c5421c970>\n",
            "tensor(28.)\n",
            "<MulBackward0 object at 0x7e8c5428b040>\n",
            "True\n",
            "False\n",
            "tensor([1., 1., 1.])\n",
            "tensor([2., 2., 2.])\n",
            "tensor(5.)\n",
            "None\n",
            "False\n",
            "torch.Size([4, 3])\n",
            "torch.Size([2, 3])\n",
            "(tensor(48.),)\n",
            "tensor(6., grad_fn=<MulBackward0>)\n",
            "tensor(2.)\n",
            "tensor([-0.4941,  0.6833,  0.7716])\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#retain graph\n",
        "\n",
        "# Standard Usage syntax: loss.backward(retain_graph=True)\n",
        "'''its use is: retain_graph=True tells PyTorch: \"Calculate the gradients,\n",
        " but keep the graph structure in memory because I need to traverse it again.\"'''\n",
        "\n",
        "x = torch.tensor([1.0], requires_grad=True)\n",
        "w = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "y = x * w\n",
        "\n",
        "loss1 = y * 2\n",
        "loss2 = y ** 2\n",
        "\n",
        "# We need to keep the graph of 'y = x * w' alive for loss2!\n",
        "loss1.backward(retain_graph=True)\n",
        "\n",
        "# Now we can destroy the graph, so retain_graph=False (default)\n",
        "loss2.backward()\n",
        "\n",
        "print(f\"Gradient of x: {x.grad}\")\n",
        "# Result: loss1 grad (2*w=4) + loss2 grad (2*y*w = 2*2*2 = 8) = 12"
      ],
      "metadata": {
        "id": "U4-mo7J2HPTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5045651-eb7a-42be-c8f4-f84d7d0537b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of x: tensor([12.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataLoader's"
      ],
      "metadata": {
        "id": "7X3iB8L4Fwiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "'''\n",
        "Dataset is of 2 types\n",
        "\n",
        "A) Map-style Datasets:\n",
        "  This is what you will use 95% of the time. It maps an index (like 0, 1, 2) to a specific data sample.\n",
        "  Syntax Requirement: You must implement two methods:\n",
        "\n",
        "    .)__len__(self): Tells PyTorch \"how many items do we have?\"\n",
        "\n",
        "    .)__getitem__(self, idx): Tells PyTorch \"give me the item at index idx.\"\n",
        "\n",
        "B)Iterable-style Datasets:\n",
        "  Used for streaming data where you can't just say \"give me item #500\" because the data is coming in a stream (like reading a huge file line-by-line or a video stream).\n",
        "\n",
        "  Syntax Requirement: You implement __iter__(self).\n",
        "'''\n",
        "\n",
        "#Map-style Datasets\n",
        "class MyCustomDataset(Dataset):\n",
        "  def __init__(self, data_list):\n",
        "    # We store the data in the class\n",
        "    self.data = data_list\n",
        "\n",
        "  def __len__(self):\n",
        "    #Required: Return total size\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    #Required: Return item at index idx\n",
        "    return self.data[idx]\n",
        "my_data = [10, 20, 30, 40, 50]\n",
        "dataset = MyCustomDataset(my_data)\n",
        "print(dataset[0])\n",
        "\n",
        "\n",
        "#iterable style datasets\n",
        "from torch.utils.data import IterableDataset\n",
        "class MyStreamDataset(IterableDataset):\n",
        "    def __iter__(self):\n",
        "        # Yields data one by one (like a generator)\n",
        "        for i in range(10):\n",
        "            yield i\n",
        "\n",
        "\n",
        "'''\n",
        "DataLoader engiene\n",
        "'''\n",
        "\n",
        "loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    sampler=None,\n",
        "    batch_sampler=None,\n",
        "    num_workers=1,\n",
        "    collate_fn=None,\n",
        "    pin_memory=False,\n",
        "    drop_last=False,\n",
        "    prefetch_factor=2,\n",
        "    # ... there are more, but these are the critical ones\n",
        ")\n",
        "'''\n",
        "batch_size: it controls how many samples are loaded per iteration\n",
        "\n",
        "shuffle: if True, it reshuffles the data at every epoch\n",
        "\n",
        "num_workers: it sets how many subprocesses to use for data loading\n",
        "              .)(0): (default). It loads a batch, then trains, then loads the next.\n",
        "              .)(>0): worker processes load data in background while the main processis training\n",
        "\n",
        "              Note: On Windows, setting num_workers > 0 can sometimes cause errors\n",
        "               if your code isn't inside if __name__ == '__main__': block.\n",
        "\n",
        "collate_fn is not clearly described here!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "collate_fn: crucial for NLP\n",
        "            .)it's used to merge a list of samples into a batch.\n",
        "            .)Default behavior: It simply stacks tensors (torch.stack).\n",
        "\n",
        "pin_memory: if True, data loader copies tensors into CUDA-pinned memory\n",
        "\n",
        "drop_last: if your dataset size is not divisible by the batch size, the last batch\n",
        "           will be smaller. drop_last=True discards that incomplete last batch.\n",
        "\n",
        "prefetch_factor: its related with num_workers\n",
        "                 .)works only when num_workrs>0\n",
        "                 .)When you set num_workers > 0, PyTorch creates a Queue\n",
        "                 .)It is a buffer size. Increase it if your GPU is waiting for data. Decrease it if your computer runs out of RAM.\n",
        "\n",
        "            Total batches in RAM = num_workers * prefetch_factor + 1\n",
        "'''\n",
        "\n",
        "#tensor dataset - Wraps tensors into a dataset immediately.\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "x = torch.randn(100,3)\n",
        "y = torch.randint(0,2,(100,))\n",
        "dataset = TensorDataset(x,y)\n",
        "loader = DataLoader(dataset, batch_size=16)\n",
        "\n",
        "#concatDataset - combines two datasets\n",
        "from torch.utils.data import ConcatDataset\n",
        "\n",
        "ds1 = TensorDataset(x[:50], y[:50])\n",
        "ds2 = TensorDataset(x[50:], y[50:])\n",
        "combined = ConcatDataset([ds1, ds2]) # Now has 100 samples again\n",
        "\n",
        "#subset and random_split\n",
        "from torch.utils.data import random_split\n",
        "# Split dataset into 80% train, 20% val\n",
        "train_ds, val_ds = random_split(dataset, [80, 20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4uHwY353SrV",
        "outputId": "cd5fb113-bfa0-4938-b561-8340306be882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "optimizer"
      ],
      "metadata": {
        "id": "zCDiu9nBYLK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "What does torch.optim even do?\n",
        "\n",
        "When you train a neural network (or any model), the goal is to reduce loss.\n",
        "Optimizers in PyTorch are tools that help adjust the model’s parameters so that loss goes down over time.\n",
        "They do this by using calculated gradients (from backprop) to update weights in a smarter way.\n",
        "\n",
        "# 1. Zero out old gradients\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# 2. Forward pass: compute predictions\n",
        "predictions = model(inputs)\n",
        "\n",
        "# 3. Compute loss\n",
        "loss = loss_fn(predictions, targets)\n",
        "\n",
        "# 4. Backward pass: compute gradients\n",
        "loss.backward()\n",
        "\n",
        "# 5. Update weights using optimizer\n",
        "optimizer.step()\n",
        "'''\n",
        "\n",
        "#to create an optimizer\n",
        "'''\n",
        "every optimizer needs\n",
        ".)parameters\n",
        ".)learning rate(lr) - how big step to take toward reducing loss\n",
        "'''\n",
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Linear(10, 1)   # a simple linear model\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "#common optimizers to use are SGD, SGD with mommentum, ADAM, RMSprop\n",
        "\n",
        "\n",
        "#SGD momentum:\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "#ADAM\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "#learning rate and adjustment\n",
        "'''\n",
        "The learning rate (lr) is a crucial hyperparameter.\n",
        " Too high → training unstable; too low → training slow.\n",
        "\n",
        "PyTorch also lets you use learning-rate schedulers that change the lr over\n",
        "time during training, which often improves final results.\n",
        "'''\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)#reduces lr every 30 eopchs\n",
        "epochs = 60\n",
        "for epoch in range(epochs):\n",
        "    scheduler.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiCjNzOAYKrs",
        "outputId": "11a897b6-9164-43fa-af3b-b8c820100395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FeedForward neural networks"
      ],
      "metadata": {
        "id": "GdFytxltDdgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#general way of writing {y = x.w(T) + b}\n",
        "# 1. Data (Inputs and Targets)\n",
        "# \"requires_grad=False\" because we don't update data, we update weights.\n",
        "x = torch.tensor([[1.0, 2.0]], requires_grad=False)  # Shape (1, 2)\n",
        "y_target = torch.tensor([[10.0]], requires_grad=False)\n",
        "\n",
        "# 2. Weights and Bias (The Learnable Parameters)\n",
        "# \"requires_grad=True\" tells PyTorch to track operations on these tensors for backprop.\n",
        "w = torch.tensor([[0.5, -0.5]], requires_grad=True)\n",
        "b = torch.tensor([0.1], requires_grad=True)\n",
        "\n",
        "# 3. Forward Pass (The Architecture)\n",
        "# Math: y = x @ w.T + b\n",
        "y_pred = x @ w.T + b\n",
        "print(f\"Prediction: {y_pred.item():.4f}\")\n",
        "\n",
        "# 4. Loss Calculation\n",
        "# MSE: (y_pred - y_target)^2\n",
        "loss = (y_pred - y_target) ** 2\n",
        "print(f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 5. Backward Pass (The Magic)\n",
        "# This calculates dLoss/dw and dLoss/db and stores them in w.grad and b.grad\n",
        "loss.backward()\n",
        "\n",
        "print(f\"Gradient dL/dw: {w.grad}\")\n",
        "print(f\"Gradient dL/db: {b.grad}\")\n",
        "\n",
        "# 6. Optimization (The Update)\n",
        "# w_new = w_old - learning_rate * gradient\n",
        "# We wrap in \"no_grad()\" because this update step shouldn't be part of the gradient graph.\n",
        "with torch.no_grad():\n",
        "    w -= 0.01 * w.grad\n",
        "    b -= 0.01 * b.grad\n",
        "\n",
        "    # Crucial: Reset gradients to zero for the next loop, otherwise they accumulate.\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "#Use the standard nn.Module and Optimizer patterns used in industry.\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# 1. Define the Architecture Class\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # nn.Linear automatically creates weights (w) and bias (b) for you\n",
        "        # and initializes them randomly.\n",
        "        self.layer1 = nn.Linear(in_features=2, out_features=4)\n",
        "        self.layer2 = nn.Linear(in_features=4, out_features=1)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        # Define the flow of data\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "# 2. Setup\n",
        "model = SimpleNet()\n",
        "\n",
        "criterion = nn.MSELoss()  # Pre-built Mean Squared Error\n",
        "\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01) # Handles the \"w -= lr * grad\" logic\n",
        "\n",
        "# 3. The Training Loop (Memorize this pattern)\n",
        "inputs = torch.randn(10, 2) # Batch of 10 samples\n",
        "targets = torch.randn(10, 1)\n",
        "for epoch in range(5):\n",
        "    # A. Reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    # B. Forward pass\n",
        "    outputs = model(inputs)\n",
        "    # C. Compute loss\n",
        "    loss = criterion(outputs, targets)\n",
        "    # D. Backward pass (computes gradients)\n",
        "    loss.backward()\n",
        "    # E. Update weights\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "#research level implementaion\n",
        "\n",
        "#1.)Custom Layer (The \"Swish\" Activation)\n",
        "class CustomSwish(nn.Module):\n",
        "    def __init__(self, beta=1.0):\n",
        "        super().__init__()\n",
        "        # \"nn.Parameter\" makes 'beta' a learnable weight.\n",
        "        # If we just used a float, PyTorch would treat it as a constant.\n",
        "        self.beta = nn.Parameter(torch.tensor([beta]))\n",
        "    def forward(self, x):\n",
        "        # Swish: x * sigmoid(beta * x)\n",
        "        return x * torch.sigmoid(self.beta * x)\n",
        "\n",
        "#2.)The Advanced Model Container\n",
        "class DeepResNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            CustomSwish(), # Using our custom layer\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            CustomSwish()\n",
        "        )\n",
        "        self.head = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        # RESEARCHER TRICK: Custom Initialization\n",
        "        # Loop through all modules and apply specific init logic\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Use Xavier initialization for better convergence\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # RESEARCHER TRICK: Residual Connection\n",
        "        # x_new = F(x) + x\n",
        "        features = self.feature_extractor(x)\n",
        "\n",
        "        # Skip connection (simple addition) requires dimensions to match\n",
        "        # Let's assume input_dim == hidden_dim for this example\n",
        "        if x.shape == features.shape:\n",
        "            features = features + x\n",
        "\n",
        "        return self.head(features)\n",
        "\n",
        "\n",
        "#Debugging & Inspection (The \"Why isn't it working?\" Phase)\n",
        "model = DeepResNet(input_dim=10, hidden_dim=10)\n",
        "inputs = torch.randn(5, 10)\n",
        "output = model(inputs)\n",
        "loss = output.mean()\n",
        "loss.backward()\n",
        "\n",
        "# TRICK: Inspecting Gradients\n",
        "# If these are 0 or NaN, your network is dying/exploding.\n",
        "for name, param in model.named_parameters():\n",
        "    if param.grad is not None:\n",
        "        grad_norm = param.grad.norm().item()\n",
        "        print(f\"Layer: {name} | Gradient Norm: {grad_norm:.6f}\")\n",
        "\n",
        "        # Gradient Clipping (prevents explosion)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)"
      ],
      "metadata": {
        "id": "lFs01BRWGDsC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c388c2b-37d6-40ce-9e13-c2de148ab18b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: -0.4000\n",
            "Loss: 108.1600\n",
            "Gradient dL/dw: tensor([[-20.8000, -41.6000]])\n",
            "Gradient dL/db: tensor([-20.8000])\n",
            "Epoch 1, Loss: 0.4355\n",
            "Epoch 2, Loss: 0.4330\n",
            "Epoch 3, Loss: 0.4306\n",
            "Epoch 4, Loss: 0.4282\n",
            "Epoch 5, Loss: 0.4262\n",
            "Layer: feature_extractor.0.weight | Gradient Norm: 1.079492\n",
            "Layer: feature_extractor.0.bias | Gradient Norm: 0.154606\n",
            "Layer: feature_extractor.1.beta | Gradient Norm: 0.019144\n",
            "Layer: feature_extractor.2.weight | Gradient Norm: 0.351044\n",
            "Layer: feature_extractor.2.bias | Gradient Norm: 0.249840\n",
            "Layer: feature_extractor.3.beta | Gradient Norm: 0.079616\n",
            "Layer: head.weight | Gradient Norm: 0.710200\n",
            "Layer: head.bias | Gradient Norm: 0.359185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PyTorch \"Underscore Rule\": Any function ending in an underscore (like ones_ or uniform_)\n",
        "modifies the tensor in-place. It doesn't return a new tensor; it overwrites the\n",
        "existing numbers in memory.\n",
        "'''"
      ],
      "metadata": {
        "id": "0G7oLtbZYik2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9fbe2164-0d16-4ff3-fe2a-74bf0a218d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPyTorch \"Underscore Rule\": Any function ending in an underscore (like ones_ or uniform_)\\nmodifies the tensor in-place. It doesn\\'t return a new tensor; it overwrites the\\nexisting numbers in memory.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.nn.init\n",
        "'''\n",
        "torch.nn.init is used to control how a neural network starts learning by\n",
        " deciding how weights and biases are initialized before training begins.\n",
        "'''\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "# Create a \"blank\" tensor. It contains whatever random garbage was in your RAM.\n",
        "w = torch.empty(3, 5)\n",
        "\n",
        "# Fill every single cell in that 3x5 matrix with the number 1.0.\n",
        "init.ones_(w)\n",
        "\n",
        "# Overwrite it: Fill everything with 0.0.\n",
        "init.zeros_(w)\n",
        "\n",
        "# Fill everything with exactly 0.42.\n",
        "init.constant_(w, 0.42)\n",
        "\n",
        "# Pick numbers randomly between 0 and 1 (Uniform distribution).\n",
        "init.uniform_(w, a=0, b=1)\n",
        "\n",
        "# Pick numbers from a Bell Curve (Mean 0, Standard Deviation 0.01).\n",
        "# This is common for \"stable\" starts.\n",
        "init.normal_(w, mean=0, std=0.01)\n",
        "\n",
        "# Apply to a real layer: Access the '.weight' attribute of the layer.\n",
        "layer = nn.Linear(1, 5)\n",
        "init.xavier_uniform_(layer.weight)\n",
        "\n",
        "\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "# This function is a 'template' that will be run on every layer of your model.\n",
        "def init_weights(self, m):\n",
        "    # Check: \"Is the current layer a Linear (Fully Connected) layer?\"\n",
        "    if isinstance(m, nn.Linear):\n",
        "        # Apply 'He Initialization' (Kaiming) designed for ReLU activations.\n",
        "        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        # If the layer has a bias (the '+ b' in y=wx+b), set it to 0.\n",
        "        # It is standard practice to start biases at zero.\n",
        "        if m.bias is not None:\n",
        "            init.zeros_(m.bias)\n",
        "# .apply() tells PyTorch: \"Go through every layer in 'model' and run 'init_weights' on it.\"\n",
        "\n",
        "#model.apply(model.init_weights)"
      ],
      "metadata": {
        "id": "i3TffKWyMY60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cnn\n",
        "import torch.nn as nn\n",
        "\n",
        "# B = Batch Size, C = Channels, H = Height, W = Width\n",
        "input_tensor = torch.randn(1, 3, 64, 64)  # A single RGB image of size 64x64\n",
        "\n",
        "# The Layer\n",
        "# in_channels=3 (RGB input)\n",
        "# out_channels=16 (We want to find 16 different types of features)\n",
        "# kernel_size=3 (The flashlight is a 3x3 grid)\n",
        "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
        "\n",
        "# The Forward Pass\n",
        "output = conv_layer(input_tensor)\n",
        "\n",
        "print(f\"Input Shape:  {input_tensor.shape}\")\n",
        "print(f\"Output Shape: {output.shape}\")\n",
        "# Output will be [1, 16, 62, 62]. Why 62? We lost pixels at the borders.\\\n",
        "\n",
        "\n",
        "# we wil add padding\n",
        "# padding=1 preserves spatial dimensions for a 3x3 kernel\n",
        "conv_preserve = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "out = conv_preserve(input_tensor)\n",
        "# Output Shape: [1, 16, 64, 64] -> Exact match to input height/width\n",
        "\n",
        "\n",
        "#we will perform stride\n",
        "# stride=2 cuts dimensions in half\n",
        "conv_downsample = nn.Conv2d(3, 16, kernel_size=3, padding=1, stride=2)\n",
        "out = conv_downsample(input_tensor)\n",
        "# Output Shape: [1, 16, 32, 32]\n",
        "\n",
        "\n",
        "#perform dilation\n",
        "# dilation=2 expands the reach\n",
        "conv_dilated = nn.Conv2d(3, 16, kernel_size=3, dilation=2)\n",
        "\n",
        "\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "#dont feel if you did'nt understand\n",
        "#reaearcher level approch\n",
        "class BottleneckBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        mid_channels = out_channels // 4 # Compress by 4x\n",
        "\n",
        "        # 1. Pointwise Conv: Squeeze channels\n",
        "        # (Reduces computation for the next step)\n",
        "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_channels)\n",
        "\n",
        "        # 2. Spatial Conv: The actual feature extraction\n",
        "        # Note the padding=1 to preserve size\n",
        "        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3,\n",
        "                               padding=1, stride=stride, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(mid_channels)\n",
        "\n",
        "        # 3. Pointwise Conv: Expand channels back\n",
        "        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x # Store for skip connection\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        # In a real ResNet, you handle shape mismatch here if stride > 1\n",
        "        if out.shape == residual.shape:\n",
        "             out += residual\n",
        "\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Verification\n",
        "block = BottleneckBlock(in_channels=256, out_channels=256)\n",
        "x = torch.randn(2, 256, 32, 32)\n",
        "print(block(x).shape) # torch.Size([2, 256, 32, 32])"
      ],
      "metadata": {
        "id": "eZiN9jqdY79a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62cd9f41-8c15-4965-82b9-603713e1045b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape:  torch.Size([1, 3, 64, 64])\n",
            "Output Shape: torch.Size([1, 16, 62, 62])\n",
            "torch.Size([2, 256, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generative cnn architecture\n",
        "\n",
        "\n",
        "#upsampling\n",
        "\n",
        "#Method - 1\n",
        "# Input: 2x2 Image\n",
        "x = torch.arange(1, 5).float().view(1, 1, 2, 2)\n",
        "# Double the size\n",
        "up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "out = up(x)\n",
        "# Output is now 4x4. The network didn't \"learn\" anything here, just math.\n",
        "\n",
        "#Method - 2\n",
        "#To double the size of an image (common in U-Net decoders), the standard setting is kernel_size=2 and stride=2.\n",
        "# The \"Un-Pooling\" Layer\n",
        "# in_channels=16 (high level features)\n",
        "# out_channels=8 (reducing depth as we grow spatial size)\n",
        "t_conv = nn.ConvTranspose2d(\n",
        "    in_channels=16,\n",
        "    out_channels=8,\n",
        "    kernel_size=2,\n",
        "    stride=2\n",
        ")\n",
        "input_tensor = torch.randn(1, 16, 32, 32)\n",
        "output = t_conv(input_tensor)\n",
        "print(f\"Input: {input_tensor.shape}\")   # [1, 16, 32, 32]\n",
        "print(f\"Output: {output.shape}\")        # [1, 8, 64, 64] -> DOUBLED\n",
        "\n",
        "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "\n",
        "#advanced level\n",
        "\n",
        "'''\n",
        "The Problem: ConvTranspose2d often creates ugly \"checkerboard\" patterns in generated images.\n",
        "Why? If the kernel size is not divisible by the stride, the \"stamps\" overlap unevenly. Some pixels get painted twice, some once. This creates a grid artifact.\n",
        "The Solution (The \"Resize-Conv\" Pattern): Instead of using ConvTranspose2d, modern researchers (like in Stable Diffusion or modern U-Nets) often prefer a two-step process:\n",
        "  .)Upsample (using nn.Upsample with 'nearest' or 'bilinear') to grow the size.\n",
        "  .)Conv2d (standard convolution) to refine the \"blurry\" upsampled features.\n",
        "This eliminates artifacts because the upsampling is smooth, and the convolution fixes the details.\n",
        "'''\n",
        "\n",
        "class UpSampleBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        # Step 1: Force growth (no parameters)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "\n",
        "        # Step 2: Learn features (parameters)\n",
        "        # We use padding=1 to keep the new size constant\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.upsample(x) # 32x32 -> 64x64\n",
        "        x = self.conv(x)     # Refine features\n",
        "        return x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-kCCdrAuiwC",
        "outputId": "b0a7b288-5a75-49eb-d5d8-cdc120678853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: torch.Size([1, 16, 32, 32])\n",
            "Output: torch.Size([1, 8, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# flattening(a old and un-preffed way)\n",
        "# Simulation of a CNN output (Batch=1, Channels=2048, H=7, W=7)\n",
        "features = torch.randn(1, 2048, 7, 7)\n",
        "\n",
        "# The Flatten Approach\n",
        "flatten = nn.Flatten()\n",
        "flat_out = flatten(features)\n",
        "\n",
        "print(f\"Flatten Output: {flat_out.shape}\")\n",
        "# Output: [1, 100352]\n",
        "\n",
        "\n",
        "#GAP(global average pooling)\n",
        "# The GAP Approach\n",
        "# PyTorch idiomatic way: AdaptiveAvgPool2d\n",
        "# \"Adaptive\" means: \"I don't care what the input size is, give me a 1x1 output.\"\n",
        "gap_layer = nn.AdaptiveAvgPool2d((1, 1))\n",
        "gap_out = gap_layer(features)\n",
        "\n",
        "print(f\"GAP Output (Raw): {gap_out.shape}\")  # [1, 2048, 1, 1]\n",
        "\n",
        "# Remove the extra 1x1 dimensions\n",
        "final_vector = torch.flatten(gap_out, 1)\n",
        "print(f\"GAP Output (Vector): {final_vector.shape}\") # [1, 2048]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLr-J_UR6IwA",
        "outputId": "d9890337-f175-497b-f724-f2fbc895f376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flatten Output: torch.Size([1, 100352])\n",
            "GAP Output (Raw): torch.Size([1, 2048, 1, 1])\n",
            "GAP Output (Vector): torch.Size([1, 2048])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization"
      ],
      "metadata": {
        "id": "T4cxDWERH9QJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Normalization forces the input of every layer to have a consistent\n",
        "mean (usually 0) and variance (usually 1). This smooths the\n",
        "optimization landscape.\n",
        "\n",
        "'''\n",
        "\n",
        "#BatchNorm-best for CNN\n",
        "\n",
        "# Input: Batch of 32 images, 3 color channels, 64x64 pixels\n",
        "input_data = torch.randn(32, 3, 64, 64)\n",
        "# BatchNorm2d is used for 4D inputs (N, C, H, W)\n",
        "# We only specify the number of channels (3)\n",
        "bn = nn.BatchNorm2d(num_features=3)\n",
        "output = bn(input_data)\n",
        "print(output.shape) # Still (32, 3, 64, 64)\n",
        "# The mean of the output across the batch (dim 0) will be approx 0\n",
        "print(output[:, 0, :, :].mean())\n",
        "\n",
        "\n",
        "#layer normalization - Best for NLP\n",
        "\n",
        "# Input: Batch of 2 sentences, 10 words each, embedding size 512\n",
        "# Shape: (N, Sequence_Length, Embedding_Dim)\n",
        "nlp_input = torch.randn(2, 10, 512)\n",
        "# We normalize over the last dimension (512)\n",
        "ln = nn.LayerNorm(normalized_shape=512)\n",
        "output = ln(nlp_input)\n",
        "# The mean is 0 across the embedding dimension (dim 2), not the batch\n",
        "print(output[0, 0, :].mean())\n",
        "\n",
        "\n",
        "#GroupNorm - best for object detection\n",
        "\n",
        "input_data = torch.randn(4, 32, 128, 128) # 32 channels\n",
        "# Split 32 channels into 8 groups (4 channels per group)\n",
        "gn = nn.GroupNorm(num_groups=8, num_channels=32)\n",
        "output = gn(input_data)\n",
        "\n",
        "\n",
        "#RMSnorm - best for modern LLM\n",
        "\n",
        "embedding_dim = 512\n",
        "rms = nn.RMSNorm(embedding_dim)\n",
        "x = torch.randn(2, 10, 512)\n",
        "output = rms(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ihkWKe7_08J",
        "outputId": "f2c16712-8b8b-4f01-c8af-9e265fe79da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 64, 64])\n",
            "tensor(-2.9104e-11, grad_fn=<MeanBackward0>)\n",
            "tensor(-3.7253e-09, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.nn.LayerNorm\n",
        "\n",
        "'''What is Layer Normalization? Layer Normalization (LayerNorm) normalizes the\n",
        "input within a specific layer for each sample independently. It makes the data\n",
        "have a mean of 0 and a standard deviation of 1\n",
        "\n",
        "It is the standard normalization used in Transformers (BERT, GPT) and RNNs.\n",
        "\n",
        "Basic Syntax - torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True)\n",
        "\n",
        "'''\n",
        "\n",
        "#hello world\n",
        "\n",
        "input_tensor = torch.tensor([[1.0, 2.0, 3.0, 4.0, 5.0]])\n",
        "# 2. Define LayerNorm\n",
        "# normalized_shape=5 means we want to normalize across the last 5 dimensions\n",
        "layer_norm = nn.LayerNorm(normalized_shape=5)\n",
        "# 3. Apply it\n",
        "output = layer_norm(input_tensor)\n",
        "\n",
        "\n",
        "#handling dimensions - NLP\n",
        "batch_size = 2\n",
        "seq_len = 3\n",
        "embed_dim = 4\n",
        "input_data = torch.randn(batch_size, seq_len, embed_dim)\n",
        "# We want to normalize over the last dimension (size 4)\n",
        "# This calculates 1 mean and 1 std per word.\n",
        "ln = nn.LayerNorm(normalized_shape=embed_dim)\n",
        "output = ln(input_data)\n",
        "print(f\"Input shape: {input_data.shape}\")\n",
        "print(f\"Output shape: {output.shape}\") # Shape remains the same\n",
        "\n",
        "\n",
        "#handling dimension - global (sometimes you may want to perform this layernorm on entire image)\n",
        "N, C, H, W = 1, 3, 32, 32\n",
        "img_data = torch.randn(N, C, H, W)\n",
        "# Normalize over Channels, Height, and Width together\n",
        "ln_image = nn.LayerNorm(normalized_shape=[C, H, W])\n",
        "out_img = ln_image(img_data)\n",
        "# Now, the Mean is calculated over all 3x32x32 pixels at once."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txnhwHEVLjHV",
        "outputId": "a9f2202e-4d63-44cf-fe7a-7d7fca26233c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 3, 4])\n",
            "Output shape: torch.Size([2, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeeding's"
      ],
      "metadata": {
        "id": "UBpn06eTc40R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "basic syntax:\n",
        "  torch.nn.Embedding(num_embeddings, embedding_dim)\n",
        "\n",
        "  .)num_embeddings: Size of the dictionary (e.g., Vocabulary size: 1000 words).\n",
        "  .)embedding_dim: Size of the vector you want for each word (e.g., 64, 128, 512).\n",
        "\n",
        "It is crucial to understand that nn.Embedding does not perform\n",
        "matrix multiplication (like a Linear layer). It performs array indexing.\n",
        "'''\n",
        "\n",
        "#hello world code\n",
        "\n",
        "# 1. Setup the layer\n",
        "# vocab_size = 5 (Indices 0, 1, 2, 3, 4)\n",
        "# dim = 3 (Each ID becomes a list of 3 numbers)\n",
        "embedding = nn.Embedding(num_embeddings=5, embedding_dim=3)\n",
        "# 2. Define input indices (Must be Integers/Longs)\n",
        "# Let's say we want the vectors for word_id 1 and word_id 4\n",
        "input_indices = torch.tensor([1, 4])\n",
        "# 3. Pass through layer\n",
        "output = embedding(input_indices)\n",
        "print(\"Input:\", input_indices)\n",
        "print(\"Output:\\n\", output)\n",
        "print(\"Output Shape:\", output.shape)\n",
        "# Shape will be [2, 3] -> [Number of Inputs, Embedding Dim]\n",
        "\n",
        "\n",
        "#weights in embeeding\n",
        "\n",
        "# Access the raw weight matrix\n",
        "# Shape: [5, 3]\n",
        "print(\"All Weights:\\n\", embedding.weight)\n",
        "# Let's verify that the embedding layer is just doing a lookup\n",
        "# We manually pick row 1 from the weights\n",
        "manual_selection = embedding.weight[1]\n",
        "# We use the layer to get index 1\n",
        "layer_selection = embedding(torch.tensor(1))\n",
        "print(\"\\nAre they the same?\")\n",
        "print(torch.equal(manual_selection, layer_selection)) # True\n",
        "\n",
        "\n",
        "#handling padding\n",
        "\n",
        "#The padding_idx argument tells PyTorch: \"Keep the vector at this index as Zeros and do not update it.\"\n",
        "# vocab_size=10, dim=4, padding_index is 0\n",
        "embedding_padded = nn.Embedding(10, 4, padding_idx=0)\n",
        "# Input batch: Two sentences\n",
        "# Sentence 1: [1, 2, 3] (Length 3)\n",
        "# Sentence 2: [4, 0, 0] (Length 1, padded with two 0s)\n",
        "input_batch = torch.tensor([\n",
        "    [1, 2, 3],\n",
        "    [4, 0, 0]\n",
        "])\n",
        "out = embedding_padded(input_batch)\n",
        "print(\"Vector for index 4 (Normal word):\\n\", out[1][0])\n",
        "print(\"Vector for index 0 (Pad token):\\n\", out[1][1])\n",
        "# You will see the pad vector is all Zeros."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vXg5d1vZmpT",
        "outputId": "5bac988d-0bd6-404a-970c-6a8933afbdbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: tensor([1, 4])\n",
            "Output:\n",
            " tensor([[-0.1378,  0.7484, -0.0948],\n",
            "        [-0.9935,  0.3337, -1.4064]], grad_fn=<EmbeddingBackward0>)\n",
            "Output Shape: torch.Size([2, 3])\n",
            "All Weights:\n",
            " Parameter containing:\n",
            "tensor([[-0.0975,  0.5610,  1.1065],\n",
            "        [-0.1378,  0.7484, -0.0948],\n",
            "        [-2.0776, -0.6813,  0.0305],\n",
            "        [-0.8480,  0.2581,  0.3481],\n",
            "        [-0.9935,  0.3337, -1.4064]], requires_grad=True)\n",
            "\n",
            "Are they the same?\n",
            "True\n",
            "Vector for index 4 (Normal word):\n",
            " tensor([-0.4762,  0.5257,  0.1441, -0.5528], grad_fn=<SelectBackward0>)\n",
            "Vector for index 0 (Pad token):\n",
            " tensor([0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers"
      ],
      "metadata": {
        "id": "oVQvELOXwtIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Before the model thinks, it needs to represent the data.\n",
        "A Transformer expects an input of shape (Batch_Size, Seq_Len, Dim).\n",
        "'''\n",
        "\n",
        "#posistional encoding\n",
        "\n",
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        # Create a matrix of [max_len, d_model] representing positional encodings\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply sine to even indices, cosine to odd indices\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add a batch dimension: [1, max_len, d_model]\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # register_buffer tells PyTorch this is a state, but not a learnable parameter\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, seq_len, d_model]\n",
        "        # We add positional encoding up to the current sequence length\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ],
      "metadata": {
        "id": "LKMolN0xd8R9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}